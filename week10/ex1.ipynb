{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "\tdef __init__(self, learning_rate = 0.01, epoch = 2000):\n",
    "\t\tself.learning_rate = learning_rate\n",
    "\t\tself.epoch = epoch\n",
    "\t\tself.w = []\n",
    "\t\tself.b = 0\n",
    "\n",
    "\tdef initialize_weigth(self, dim):\n",
    "\t\tw = np.random.normal(0, 1, (dim, 1))\n",
    "\t\tb = np.random.rand(1)\n",
    "\t\treturn w, b\n",
    "\n",
    "\tdef sigmoid(self, x):\n",
    "\t\treturn 1 / (1 + np.exp(-x))\n",
    "\n",
    "\tdef hypothesis(self, w, X, b):\n",
    "\t\ty_hat = self.sigmoid(np.matmul(X, w) + b)\n",
    "\t\ty_hat = np.squeeze(y_hat)\n",
    "\t\treturn y_hat\n",
    "\t\n",
    "\tdef cost(self, y_hat, y, N):\n",
    "\t\tcost = -(1 / N) * np.sum(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n",
    "\t\tcost = np.squeeze(cost)\n",
    "\t\treturn cost\n",
    "\n",
    "\tdef cal_gradient(self, w, y_hat, X, y):\n",
    "\t\tN = X.shape[0]\n",
    "\t\tdelta_w = (1 / N) * np.matmul(X.T, (y_hat - y))\n",
    "\t\tdelta_b = (1 / N) * np.sum(y_hat - y)\n",
    "\t\tgrads = {\n",
    "\t\t\t\"delta_w\": delta_w,\n",
    "\t\t\t\"delta_b\": delta_b\n",
    "\t\t}\n",
    "\t\treturn grads\n",
    "\n",
    "\tdef gradient_position(self, w, b, X, y):\n",
    "\t\tN = X.shape[0]\n",
    "\t\ty_hat = self.hypothesis(w, X, b)\n",
    "\t\tcost = self.cost(y_hat, y, N)\n",
    "\t\tgrads = self.cal_gradient(w, y_hat, X, y)\n",
    "\t\treturn grads, cost\n",
    "\n",
    "\tdef gradient_descent(self, w, b, X, y, print_cost = False):\n",
    "\t\tcosts = []\n",
    "\t\tfor i in range(self.epoch):\n",
    "\t\t\tgrads, cost = self.gradient_position(w, b, X, y)\n",
    "\n",
    "\t\t\tdelta_w = grads[\"delta_w\"]\n",
    "\t\t\tdelta_b = grads[\"delta_b\"]\n",
    "\n",
    "\t\t\tdelta_w = delta_w.reshape(-1, 1)\n",
    "\n",
    "\t\t\tw = w - (self.learning_rate * delta_w)\n",
    "\t\t\tb = b - (self.learning_rate * delta_b)\n",
    "\t\t\tif i % 100 == 0:\n",
    "\t\t\t\tcosts.append(cost)\n",
    "\n",
    "\t\t\tif print_cost and i % 100 == 0:\n",
    "\t\t\t\tprint(\"Cost after iteration %i: %f\" %(i, cost))\n",
    "\t\t\t\n",
    "\t\t\tparams = {\n",
    "\t\t\t\t\"w\": w,\n",
    "\t\t\t\t\"b\": b\n",
    "\t\t\t}\n",
    "\t\t\tgrads = {\n",
    "\t\t\t\t\"delta_w\": delta_w,\n",
    "\t\t\t\t\"delta_b\": delta_b\n",
    "\t\t\t}\n",
    "\n",
    "\t\treturn params, costs\n",
    "\n",
    "\tdef predict(self, X):\n",
    "\t\tX = np.array(X)\n",
    "\t\tN = X.shape[0]\n",
    "\n",
    "\t\tY_prediction = np.zeros(N)\n",
    "\n",
    "\t\tw = self.w.reshape(X.shape[1], 1)\n",
    "\t\tb = self.b\n",
    "\n",
    "\t\ty_hat = self.hypothesis(w, X, b)\n",
    "\t\tfor i in range(len(y_hat)):\n",
    "\t\t\tif y_hat[i] >= 0.5:\n",
    "\t\t\t\tY_prediction[i] = 1\n",
    "\t\t\telse:\n",
    "\t\t\t\tY_prediction[i] = 0\n",
    "\t\t\n",
    "\t\treturn Y_prediction\n",
    "\n",
    "\tdef train_model(self, X_train, Y_train, X_test, Y_test, print_cost = False):\n",
    "\t\tdim = np.shape(X_train)[1]\n",
    "\t\tw, b = self.initialize_weigth(dim)\n",
    "\t\tparameters, costs = self.gradient_descent(w, b, X_train, Y_train, print_cost=False)\n",
    "\n",
    "\t\tself.w = parameters['w']\n",
    "\t\tself.b = parameters['b']\n",
    "\n",
    "\t\tY_prediction_test = self.predict(X_test)\n",
    "\t\tY_prediction_train = self.predict(X_train)\n",
    "\n",
    "\t\ttrain_score = 100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100\n",
    "\t\ttest_scroe = 100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100\n",
    "\t\tprint(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "\n",
    "\t\tresult_dict = {\n",
    "\t\t\t\"costs\": costs,\n",
    "\t\t\t\"Y_prediction_test\": Y_prediction_test,\n",
    "\t\t\t\"Y_prediction_train\": Y_prediction_train,\n",
    "\t\t\t\"w\": self.w,\n",
    "\t\t\t\"b\": self.b,\n",
    "\t\t\t\"learning_rate\": self.learning_rate,\n",
    "\t\t\t\"num_iteration\": self.epoch,\n",
    "\t\t\t\"train_accuracy\": train_score,\n",
    "\t\t\t\"test_accuracy\": test_scroe\n",
    "\t\t}\n",
    "\n",
    "\t\treturn result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy_data = np.load('./data.npz')\n",
    "\n",
    "X = xy_data['x']\n",
    "Y = xy_data['y']\n",
    "\n",
    "train_number = int(Y.shape[0] * 0.9)\n",
    "\n",
    "X_train = X[:train_number, :]\n",
    "Y_train = Y[:train_number]\n",
    "\n",
    "X_test = X[train_number:, :]\n",
    "Y_test = Y[train_number:]\n",
    "\n",
    "plt.scatter(X_train[Y_train == 0][:, 0],\n",
    "            X_train[Y_train == 0][:, 1], color='red')\n",
    "plt.scatter(X_train[Y_train == 1][:, 0],\n",
    "            X_train[Y_train == 1][:, 1], color='green')\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression()\n",
    "result_dict = LR.train_model(X_train, Y_train, X_test, Y_test)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5db0bf8e65298cb01341d1d0401e9d0f1ebbad7939f704a64363986abd13b2a8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
