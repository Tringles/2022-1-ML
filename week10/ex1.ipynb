{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "\tdef __init__(self, learning_rate = 0.01, epoch = 2000):\n",
    "\t\tself.learning_rate = learning_rate\n",
    "\t\tself.epoch = epoch\n",
    "\t\tself.w = []\n",
    "\t\tself.b = 0\n",
    "\n",
    "\tdef initialize_weigth(self, dim):\n",
    "\t\tw = np.random.normal(0, 1, (dim, 1))\n",
    "\t\tb = np.random.rand(1)\n",
    "\t\treturn w, b\n",
    "\n",
    "\tdef sigmoid(self, x):\n",
    "\t\treturn 1 / (1 + np.exp(-x))\n",
    "\n",
    "\tdef hypothesis(self, w, X, b):\n",
    "\t\ty_hat = self.sigmoid(np.matmul(X, w) + b)\n",
    "\t\ty_hat = np.squeeze(y_hat)\n",
    "\t\treturn y_hat\n",
    "\t\n",
    "\tdef cost(self, y_hat, y, N):\n",
    "\t\tcost = -(1 / N) * np.sum(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n",
    "\t\tcost = np.squeeze(cost)\n",
    "\t\treturn cost\n",
    "\n",
    "\tdef cal_gradient(self, w, y_hat, X, y):\n",
    "\t\tN = X.shape[1]\n",
    "\t\tdelta_w = (1 / N) * np.matmul(X, w, (y_hat - y))\n",
    "\t\tdelta_b = (1 / N) * np.sum(y_hat - y)\n",
    "\t\tgrads = {\n",
    "\t\t\t\"delta_w\": delta_w,\n",
    "\t\t\t\"delta_b\": delta_b\n",
    "\t\t}\n",
    "\t\treturn grads\n",
    "\n",
    "\tdef gradient_position(self, w, b, X, y):\n",
    "\t\tN = X.shape[0]\n",
    "\t\ty_hat = self.hypothesis(w, X, b)\n",
    "\t\tcost = self.cost(y_hat, y, N)\n",
    "\t\tgrads = self.cal_gradient(w, y_hat, X, y)\n",
    "\t\treturn grads, cost\n",
    "\n",
    "\tdef gradient_descent(self, w, b, X, y, print_cost = False):\n",
    "\t\tcosts = []\n",
    "\t\tfor i in range(self.epoch):\n",
    "\t\t\tgrads, cost = self.gradient_position(w, b, X, y)\n",
    "\n",
    "\t\t\tdelta_w = grads[\"delta_w\"]\n",
    "\t\t\tdelta_b = grads[\"delta_b\"]\n",
    "\n",
    "\t\t\tdelta_w = delta_w.reshape(-1, 1)\n",
    "\n",
    "\t\t\tw = w - (self.learning_rate * delta_w)\n",
    "\t\t\tb = b - (self.learning_rate * delta_b)\n",
    "\t\t\tif i % 100 == 0:\n",
    "\t\t\t\tcosts.append(cost)\n",
    "\n",
    "\t\t\tif print_cost and i % 100 == 0:\n",
    "\t\t\t\tprint(\"Cost after iteration %i: %f\" %(i, cost))\n",
    "\t\t\t\n",
    "\t\t\tparams = {\n",
    "\t\t\t\t\"w\": w,\n",
    "\t\t\t\t\"b\": b\n",
    "\t\t\t}\n",
    "\t\t\tgrads = {\n",
    "\t\t\t\t\"delta_w\": delta_w,\n",
    "\t\t\t\t\"delta_b\": delta_b\n",
    "\t\t\t}\n",
    "\n",
    "\t\treturn params, costs\n",
    "\t\t\n",
    "\tdef train_model(self, X_train, Y_train, X_test, Y_test, print_cost = False):\n",
    "\t\tdim = np.shape(X_train)[1]\n",
    "\t\tw, b = self.initialize_weigth(dim)\n",
    "\t\tparameters, costs = self.gradient_descent(w, b, X_train, Y_train)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5db0bf8e65298cb01341d1d0401e9d0f1ebbad7939f704a64363986abd13b2a8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
